{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c11028-5110-4a8f-ba9b-3af4a9576c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73fac8e1-204e-4753-8f14-28324fc3c940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "import torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9abc015f-dab1-4964-b228-c00a2f12a587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "val_dataset = datasets.ImageFolder(root='./data/Imagenet/val/',\n",
    "                                    transform=test_transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869081a-160f-4c65-b0fb-6d1c52b345fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de2025f-bdbd-47ab-afc2-e6900954b546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_paths = { 'ViT': './models/road_sign/vit_b_16_0.ckpt',\n",
    "            'DenseNet121':'./models/road_sign/densenet121_0.ckpt',\n",
    "            'Efficientnet_b2': './models/road_sign/efficientnet_b2_0.ckpt',\n",
    "            'Resnet18': './models/road_sign/resnet18_0.ckpt',\n",
    "            'Swin_s': './models/road_sign/swin_s_0.ckpt'}\n",
    "                    \n",
    "\n",
    "models = {'ViT_b_16': 'vit_b_16', 'ViT_l_16': 'vit_l_16',\n",
    "          'DenseNet121': 'densenet121', 'DenseNet161': 'densenet161',\n",
    "          'Efficientnet_b2':'efficientnet_b2', 'Efficientnet_b1': 'efficientnet_b1',\n",
    "          'Resnet18': 'resnet18', 'Resnet34': 'resnet34',\n",
    "          'Swin_s': 'swin_s', 'Swin_t': 'swin_t'}\n",
    "\n",
    "model_names = ['ViT_b_16', 'DenseNet121', 'Efficientnet_b2', 'Resnet18', 'Swin_s']\n",
    "\n",
    "\n",
    "eps = 1/255\n",
    "alpha = 0.25/255\n",
    "iters = 7\n",
    "attack = 'Momentum'\n",
    "save_path = 'save_path_with_a_lot_of_memory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea512b59-77b9-408a-a480-6b11c810fbe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class model_with_normalization(nn.Module):\n",
    "    def __init__(self, model, normalization):\n",
    "        super(model_with_normalization, self).__init__()\n",
    "        self.model = model \n",
    "        self.normalization = normalization\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.normalization(x)\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92801cc3-449f-4da9-86ae-db6f30aa1a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for model_name in model_names:\n",
    "#     m = torch.hub.load(\"pytorch/vision\", models[model_name],\n",
    "#                            weights=\"IMAGENET1K_V1\").to(device)\n",
    "#     m.eval()\n",
    "#     m = model_with_normalization(m, normalization)\n",
    "#     # atk = torchattacks.PGD(m,random_start=False, eps=eps,alpha=alpha,steps=iters)\n",
    "#     atk = torchattacks.MIFGSM(m, eps=eps, steps=iters, decay=0.9)\n",
    "#     # atk = torchattacks.FGSM(m, eps=eps)\n",
    "    \n",
    "#     true_labels = []\n",
    "#     adv_inputs = []\n",
    "#     sample_indexes = []\n",
    "#     sample_num = 0\n",
    "    \n",
    "#     for batch, labels in val_loader:\n",
    "#         batch, labels = batch.to(device), labels.to(device)\n",
    "#         sample_index = torch.arange(sample_num, sample_num+labels.size(0))\n",
    "#         adv_samples = atk(batch, labels)\n",
    "#         pred_labels = m(adv_samples).argmax(dim=1)\n",
    "#         mask = pred_labels.eq(labels)\n",
    "#         if mask.all():\n",
    "#             continue\n",
    "#         else:\n",
    "#             adv_samples = adv_samples[~mask]\n",
    "#             labels = labels[~mask]\n",
    "#             sample_index = sample_index[~mask.cpu()]\n",
    "\n",
    "#             true_labels.append(labels.to('cpu'))\n",
    "#             adv_inputs.append(adv_samples.to('cpu'))\n",
    "#             sample_indexes.append(sample_index)\n",
    "#         sample_num += labels.size(0)        \n",
    "        \n",
    "#     true_labels = torch.cat(true_labels)\n",
    "#     adv_inputs = torch.cat(adv_inputs, dim=0)\n",
    "#     sample_indexes = torch.cat(sample_indexes)\n",
    "    \n",
    "#     torch.save(true_labels, os.path.join(save_path,\n",
    "#                                          f'adv_samples/{attack}/imagenet/{model_name}_labels.ckpt'))\n",
    "#     torch.save(adv_inputs,  os.path.join(save_path,\n",
    "#                                          f'adv_samples/{attack}/imagenet/{model_name}_samples.ckpt'))\n",
    "#     torch.save(sample_indexes,  os.path.join(save_path,\n",
    "#                                              f'adv_samples/{attack}/imagenet/{model_name}_index.ckpt'))\n",
    "#     del atk, m\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a9100d-440d-426f-813a-1b567c3c9d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m = torch.hub.load(\"pytorch/vision\", models['Swin_s'],\n",
    "#                        weights=\"IMAGENET1K_V1\")\n",
    "# m = modify_model_output_classes(m, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b5119b-b319-4397-9455-61035abac812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m.load_state_dict(torch.load(model_paths['Swin_s'])['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2ef19a-2ebc-403a-9380-fcde3ff239c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight = m.fc.weight[:3, :]\n",
    "# bias = m.fc.bias[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3ac1cb7-d721-4397-b1a3-35fa7ed8ca1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m.fc = nn.Linear(512, 3)\n",
    "# m.fc.weight.data = weight\n",
    "# m.fc.bias.data = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e81464ab-09d7-48fb-8a2c-33eb05538a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# state = {'net': m.state_dict()}\n",
    "# torch.save(state, './models/xray/resnet18_0.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7661631c-cf4f-412c-94e3-6079644c78a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "other_list = {'Resnet18': ['ViT_b_16','Resnet34', 'DenseNet121', 'Efficientnet_b2', 'Swin_s'],\n",
    "              'Swin_s': ['ViT_b_16', 'Swin_t', 'DenseNet121', 'Efficientnet_b2', 'Resnet18'],\n",
    "              'ViT_b_16': ['DenseNet121', 'ViT_l_16', 'Efficientnet_b2', 'Resnet18', 'Swin_s'],\n",
    "              'DenseNet121': ['ViT_b_16', 'DenseNet161', 'Efficientnet_b2', 'Resnet18', 'Swin_s'],\n",
    "              'Efficientnet_b2': ['ViT_b_16', 'Efficientnet_b1', 'DenseNet121', 'Resnet18', 'Swin_s'],            \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "586f6ff2-9c59-41d5-b0f8-5220127640b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for surogate, others in other_list.items():\n",
    "    \n",
    "#     true_labels= torch.load(os.path.join(save_path,\n",
    "#                                          f'adv_samples/{attack}/imagenet/{surogate}_labels.ckpt'))\n",
    "#     adv_inputs = torch.load(os.path.join(save_path,\n",
    "#                                          f'adv_samples/{attack}/imagenet/{surogate}_samples.ckpt'))\n",
    "#     sample_index = torch.load(os.path.join(save_path,\n",
    "#                                              f'adv_samples/{attack}/imagenet/{surogate}_index.ckpt'))\n",
    "    \n",
    "#     adv_dataset = torch.utils.data.TensorDataset(adv_inputs, true_labels, sample_index)\n",
    "#     adv_loader = torch.utils.data.DataLoader(adv_dataset, batch_size=64,\n",
    "#                                              shuffle=False, num_workers=4)\n",
    "#     print(f'Testing model {surogate}')\n",
    "#     results_df = {}\n",
    "#     for test_model_name in others:\n",
    "        \n",
    "#         results_df[f'{test_model_name}_pred_label'] = []\n",
    "#         results_df[f'{test_model_name}_pred_conf'] = []\n",
    "#         results_df[f'{test_model_name}_true_conf'] = []\n",
    "#         results_df[f'{test_model_name}_true_label'] = []\n",
    "        \n",
    "#         model = models[test_model_name]\n",
    "        \n",
    "#         m = torch.hub.load(\"pytorch/vision\", model,\n",
    "#                                weights=\"IMAGENET1K_V1\").to(device)\n",
    "#         m.eval()\n",
    "#         m = model_with_normalization(m, normalization)\n",
    "\n",
    "#         true_labels = []\n",
    "#         adv_inputs = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch, labels, _ in adv_loader:\n",
    "#                 batch = batch.to(device)\n",
    "#                 outputs = torch.softmax(m(batch), dim=1).cpu()\n",
    "#                 max_return = outputs.max(dim=1)\n",
    "                \n",
    "#                 results_df[f'{test_model_name}_pred_label'].extend(max_return.indices.numpy().tolist())\n",
    "#                 results_df[f'{test_model_name}_pred_conf'].extend(max_return.values.numpy().tolist())\n",
    "                \n",
    "#                 true_conf = torch.gather(outputs, dim=1, index=labels.view(-1, 1)).view(-1)\n",
    "#                 results_df[f'{test_model_name}_true_conf'].extend(true_conf.cpu().numpy().tolist())\n",
    "#                 results_df[f'{test_model_name}_true_label'].extend(labels.numpy().tolist())\n",
    "\n",
    "#         del m\n",
    "        \n",
    "#     results_df['sample_index'] = []\n",
    "#     for _, _, index in adv_loader:\n",
    "#         results_df['sample_index'].extend(index.numpy().tolist())\n",
    "        \n",
    "#     results_df = pd.DataFrame(results_df)\n",
    "#     results_df.to_csv(f'./results/{attack}/imagenet_surogate_{surogate}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c80dcd7c-09a3-4651-802a-5ee5f202f721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.gather(outputs, dim=1, index=labels.view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16155560-efac-4949-a038-5ba39e014445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e0b1bf-6212-4d79-a805-a9f8f818dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = {}\n",
    "\n",
    "# for model_name in model_names:\n",
    "#     if '_copy' in model_name:\n",
    "#         continue\n",
    "#     m = torch.hub.load(\"pytorch/vision\", models[model_name],\n",
    "#                            weights=\"IMAGENET1K_V1\").to(device)\n",
    "#     m.eval()\n",
    "#     m = model_with_normalization(m, normalization)\n",
    "#     print(f'loaded {model_name}')\n",
    "\n",
    "#     df[f'{model_name}_probs'] = []\n",
    "#     df[f'{model_name}_noise_probs'] = []\n",
    "#     with torch.no_grad():\n",
    "#         sample_num = 0\n",
    "#         for batch, labels in val_loader:\n",
    "#             batch, labels = batch.to(device), labels.to(device)\n",
    "#             probs = torch.softmax(m(batch), dim=1).max(dim=1).values\n",
    "#             noise_probs =torch.softmax(m(torch.clamp(batch+\n",
    "#                                                      (16/255)*torch.randn_like(batch),\n",
    "#                                                      0, 1)),\n",
    "#                                        dim=1).max(dim=1).values\n",
    "\n",
    "#             df[f'{model_name}_probs'].extend(probs.detach().cpu().numpy().tolist())\n",
    "#             df[f'{model_name}_noise_probs'].extend(noise_probs.detach().cpu().numpy().tolist())\n",
    "# df['sample_index'] = list(range(len(val_loader.dataset)))        \n",
    "# df = pd.DataFrame(df)\n",
    "# df.to_csv(f'./results/{attack}/imagenet_all_preds.csv', index=False)\n",
    "\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a36b4-f4c6-448d-8a04-77c5a147f943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a572ed-b651-4002-9092-ce071c9cb117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, org_size, inter_size):\n",
    "        self.org_size = org_size\n",
    "        self.inter_size = inter_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = transforms.functional.resize(img, self.inter_size)\n",
    "        return transforms.functional.resize(img, self.org_size)\n",
    "    \n",
    "class Compress(object):\n",
    "    def __init__(self,):\n",
    "        self.encode_param =  [int(cv2.IMWRITE_JPEG_QUALITY), 90]\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = (img.cpu().numpy()*255).astype(np.uint8).transpose([1,2,0])\n",
    "        result, encimg = cv2.imencode('.jpg', img, self.encode_param)\n",
    "        decimg = cv2.imdecode(encimg, 1)\n",
    "        return torch.Tensor(decimg.transpose(2,0,1))/255\n",
    "    \n",
    "class Adv_dataset_with_transforms(Dataset):\n",
    "    def __init__(self, adv_inputs, true_labels, sample_index, transforms):\n",
    "        self.data = adv_inputs\n",
    "        self.target = true_labels\n",
    "        self.sample_index = sample_index\n",
    "        self.transform = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = self.transform(x)\n",
    "        \n",
    "        y = self.target[index]\n",
    "        sample_index = self.sample_index[index]\n",
    "        return x, y, sample_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ad9cb43-5637-4cfb-bf0f-2919a6605131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = 'combined'\n",
    "\n",
    "if transform == 'compression':\n",
    "    trans = Compress()\n",
    "elif transform == 'resize':\n",
    "    trans = Resize(224, 200)\n",
    "elif transform == 'combined':\n",
    "    trans = transforms.Compose([\n",
    "        Compress(),\n",
    "        Resize(224, 200)\n",
    "    ])\n",
    "else:\n",
    "    trans = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6644626a-1baa-4316-9c4f-315f95406817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model Resnet18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model Swin_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model ViT_b_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model DenseNet121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model Efficientnet_b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n",
      "Using cache found in /home/guy5/.cache/torch/hub/pytorch_vision_main\n"
     ]
    }
   ],
   "source": [
    "for surogate, others in other_list.items():\n",
    "    \n",
    "    true_labels= torch.load(os.path.join(save_path,\n",
    "                                         f'adv_samples/PGD/imagenet/{surogate}_labels.ckpt'))\n",
    "    adv_inputs = torch.load(os.path.join(save_path,\n",
    "                                         f'adv_samples/PGD/imagenet/{surogate}_samples.ckpt'))\n",
    "    sample_index = torch.load(os.path.join(save_path,\n",
    "                                             f'adv_samples/PGD/imagenet/{surogate}_index.ckpt'))\n",
    "    \n",
    "    adv_dataset = Adv_dataset_with_transforms(adv_inputs, true_labels, sample_index, trans)\n",
    "    adv_loader = torch.utils.data.DataLoader(adv_dataset, batch_size=64,\n",
    "                                             shuffle=False, num_workers=4)\n",
    "    print(f'Testing model {surogate}')\n",
    "    results_df = {}\n",
    "    for test_model_name in others:\n",
    "        \n",
    "        results_df[f'{test_model_name}_pred_label'] = []\n",
    "        results_df[f'{test_model_name}_pred_conf'] = []\n",
    "        results_df[f'{test_model_name}_true_conf'] = []\n",
    "        results_df[f'{test_model_name}_true_label'] = []\n",
    "        \n",
    "        model = models[test_model_name]\n",
    "        \n",
    "        m = torch.hub.load(\"pytorch/vision\", model,\n",
    "                               weights=\"IMAGENET1K_V1\").to(device)\n",
    "        m.eval()\n",
    "        m = model_with_normalization(m, normalization)\n",
    "\n",
    "        true_labels = []\n",
    "        adv_inputs = []\n",
    "        with torch.no_grad():\n",
    "            for batch, labels, _ in adv_loader:\n",
    "                batch = batch.to(device)\n",
    "                outputs = torch.softmax(m(batch), dim=1).cpu()\n",
    "                max_return = outputs.max(dim=1)\n",
    "                \n",
    "                results_df[f'{test_model_name}_pred_label'].extend(max_return.indices.numpy().tolist())\n",
    "                results_df[f'{test_model_name}_pred_conf'].extend(max_return.values.numpy().tolist())\n",
    "                \n",
    "                true_conf = torch.gather(outputs, dim=1, index=labels.view(-1, 1)).view(-1)\n",
    "                results_df[f'{test_model_name}_true_conf'].extend(true_conf.cpu().numpy().tolist())\n",
    "                results_df[f'{test_model_name}_true_label'].extend(labels.numpy().tolist())\n",
    "\n",
    "        del m\n",
    "        \n",
    "    results_df['sample_index'] = []\n",
    "    for _, _, index in adv_loader:\n",
    "        results_df['sample_index'].extend(index.numpy().tolist())\n",
    "        \n",
    "    results_df = pd.DataFrame(results_df)\n",
    "    results_df.to_csv(f'./results/transforms/{transform}/imagenet_surogate_{surogate}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbdae5b-c0ca-4f3b-9573-9efb385e791e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
